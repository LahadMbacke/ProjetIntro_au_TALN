{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lahad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lahad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12473, 7)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prendre en main les données\n",
    "data = pd.read_csv(\"../data/train.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['doc_id', 'titre', 'type', 'difficulte', 'cout', 'ingredients',\n",
       "       'recette'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data['text'] = data['titre'] + \" \" + data['recette']\n",
    "data = data[['text', 'type']]  \n",
    "X = data['text']\n",
    "y = data['type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Dessert       0.31      0.31      0.31      1092\n",
      "        Entrée       0.25      0.24      0.25       912\n",
      "Plat principal       0.47      0.48      0.47      1738\n",
      "\n",
      "      accuracy                           0.37      3742\n",
      "     macro avg       0.34      0.34      0.34      3742\n",
      "  weighted avg       0.37      0.37      0.37      3742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Methode Baseline\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "dummy = DummyClassifier(strategy='stratified')\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dummy.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run2: TF-IDF + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8609510086455331\n",
      "Precision: 0.8577441880603762\n",
      "Recall: 0.8609510086455331\n",
      "F1 Score: 0.8585459215001946\n",
      "Confusion Matrix:\n",
      "[[403   2   2]\n",
      " [  8 225 104]\n",
      " [  4  73 567]]\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Dessert       0.97      0.99      0.98       407\n",
      "        Entrée       0.75      0.67      0.71       337\n",
      "Plat principal       0.84      0.88      0.86       644\n",
      "\n",
      "      accuracy                           0.86      1388\n",
      "     macro avg       0.85      0.85      0.85      1388\n",
      "  weighted avg       0.86      0.86      0.86      1388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "\n",
    "data['text'] = data['titre'] + \" \" + data['recette']\n",
    "data = data[['text', 'type']]  # Assurez-vous de remplacer 'label' par le nom de votre colonne de label\n",
    "X = data['text']\n",
    "y = data['type']\n",
    "\n",
    "\n",
    "\n",
    "test['text'] = test['titre']+ \" \" + test['recette']\n",
    "test = test[['text', 'type']]  # Assurez-vous de remplacer 'label' par le nom de votre colonne de label\n",
    "X_testt = test['text']\n",
    "y_testt = test['type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_testt)\n",
    "\n",
    "svm = SVC(kernel='linear', C=1, random_state=42)\n",
    "\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_testt, y_pred)\n",
    "precision = precision_score(y_testt, y_pred, average='weighted')\n",
    "recall = recall_score(y_testt, y_pred, average='weighted')\n",
    "f1 = f1_score(y_testt, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_testt, y_pred)\n",
    "class_report = classification_report(y_testt, y_pred)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run3: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8378962536023055\n",
      "Precision: 0.8336336642802019\n",
      "Recall: 0.8378962536023055\n",
      "F1 Score: 0.8291274228851641\n",
      "Confusion Matrix:\n",
      "[[404   2   1]\n",
      " [  9 179 149]\n",
      " [ 10  54 580]]\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Dessert       0.96      0.99      0.97       407\n",
      "        Entrée       0.76      0.53      0.63       337\n",
      "Plat principal       0.79      0.90      0.84       644\n",
      "\n",
      "      accuracy                           0.84      1388\n",
      "     macro avg       0.84      0.81      0.81      1388\n",
      "  weighted avg       0.83      0.84      0.83      1388\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #Modèle Word2Vec\n",
    "\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "test['text'] = test['titre']+ \" \" + test['recette']\n",
    "test = test[['text', 'type']]  # Assurez-vous de remplacer 'label' par le nom de votre colonne de label\n",
    "X_testt = test['text']\n",
    "y_testt = test['type']\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "corpus = [sentence.split() for sentence in X_train]\n",
    "word2vec_model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Calcul des vecteurs de mots moyens pour chaque document\n",
    "def document_vector(word2vec_model, doc):\n",
    "    \"\"\"Calculer le vecteur moyen pour un document en utilisant les vecteurs de mots Word2Vec\"\"\"\n",
    "    # filtrer les mots absents dans le vocabulaire\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    if len(doc) != 0:\n",
    "        return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "X_train_word2vec = np.array([document_vector(word2vec_model, doc.split()) for doc in X_train])\n",
    "X_test_word2vec = np.array([document_vector(word2vec_model, doc.split()) for doc in X_testt])\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', C=10, random_state=42)\n",
    "svm_classifier.fit(X_train_word2vec, y_train)\n",
    "\n",
    "\n",
    "y_pred_word2vec = svm_classifier.predict(X_test_word2vec)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_testt, y_pred_word2vec)\n",
    "precision = precision_score(y_testt, y_pred_word2vec, average='weighted')\n",
    "recall = recall_score(y_testt, y_pred_word2vec, average='weighted')\n",
    "f1 = f1_score(y_testt, y_pred_word2vec, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_testt, y_pred_word2vec)\n",
    "class_report = classification_report(y_testt, y_pred_word2vec)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run4: CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8566282420749279\n",
      "Precision: 0.8528220369056105\n",
      "Recall: 0.8566282420749279\n",
      "F1 Score: 0.8517733884649409\n",
      "Confusion Matrix:\n",
      "[[404   1   2]\n",
      " [  9 208 120]\n",
      " [  8  59 577]]\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Dessert       0.96      0.99      0.98       407\n",
      "        Entrée       0.78      0.62      0.69       337\n",
      "Plat principal       0.83      0.90      0.86       644\n",
      "\n",
      "      accuracy                           0.86      1388\n",
      "     macro avg       0.85      0.84      0.84      1388\n",
      "  weighted avg       0.85      0.86      0.85      1388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Modèle CountVectorizer\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "test['text'] = test['titre']+ \" \" + test['recette']\n",
    "test = test[['text', 'type']]  # Assurez-vous de remplacer 'label' par le nom de votre colonne de label\n",
    "X_testt = test['text']\n",
    "y_testt = test['type']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_testt)\n",
    "\n",
    "svm_classifier = SVC(random_state=42)\n",
    "svm_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "y_pred_counts = svm_classifier.predict(X_test_counts)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_testt, y_pred_counts)\n",
    "precision = precision_score(y_testt, y_pred_counts, average='weighted')\n",
    "recall = recall_score(y_testt, y_pred_counts, average='weighted')\n",
    "f1 = f1_score(y_testt, y_pred_counts, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_testt, y_pred_counts)\n",
    "class_report = classification_report(y_testt, y_pred_counts)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### TEST TF-IDF avec Nettotage de texte et Stemming   #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_fr = set(stopwords.words('french'))\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Supprimer les balises HTML\n",
    "    text = re.sub(r'[^a-zA-ZÀ-ÿ]', ' ', text)\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text, language='french')\n",
    "    words = [word for word in words if word not in stopwords_fr]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Prétraitement des données d'entraînement\n",
    "data['text'] = data['titre']+ \" \" + data['recette']\n",
    "data = data[['text', 'type']]\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "# data['text'] = data['text'].apply(stemmer)\n",
    "\n",
    "\n",
    "\n",
    "test['text'] = test['titre']+ \" \" + test['recette']\n",
    "test = test[['text', 'type']]\n",
    "test['text'] = test['text'].apply(clean_text)\n",
    "# test['text'] = test['text'].apply(stemmer)\n",
    "X_testt = test['text']\n",
    "y_testt = test['type']\n",
    "\n",
    "\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['type'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Vecteur TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_testt)  # Utiliser transform, pas fit_transform\n",
    "\n",
    "# Entraînement du modèle SVM\n",
    "svm = SVC(kernel='rbf', C=10, random_state=42)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_testt, y_pred)\n",
    "precision = precision_score(y_testt, y_pred, average='weighted')\n",
    "recall = recall_score(y_testt, y_pred, average='weighted')\n",
    "f1 = f1_score(y_testt, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_testt, y_pred)\n",
    "class_report = classification_report(y_testt, y_pred)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters = {'kernel':('linear', 'rbf'), 'C':[1,10]}\n",
    "# svc = SVC(random_state=42)\n",
    "# clf = GridSearchCV(svc, parameters)\n",
    "# clf.fit(X_train_tfidf, y_train)\n",
    "# #Print the best parameters\n",
    "# print(\"Best parameters: \", clf.best_params_)\n",
    "# # Use the best estimator to make predictions\n",
    "# y_pred = clf.best_estimator_.predict(X_test_tfidf)\n",
    "\n",
    "# # Evaluation\n",
    "# print(classification_report(y_testt, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8739193083573487\n",
      "Precision: 0.87126309253499\n",
      "Recall: 0.8739193083573487\n",
      "F1 Score: 0.8714596251334252\n",
      "Confusion Matrix:\n",
      "[[404   1   2]\n",
      " [  5 233  99]\n",
      " [  8  60 576]]\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Dessert       0.97      0.99      0.98       407\n",
      "        Entrée       0.79      0.69      0.74       337\n",
      "Plat principal       0.85      0.89      0.87       644\n",
      "\n",
      "      accuracy                           0.87      1388\n",
      "     macro avg       0.87      0.86      0.86      1388\n",
      "  weighted avg       0.87      0.87      0.87      1388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################################### Modèle CountVectorizer\n",
    "\n",
    "data = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Prétraitement des données d'entraînement\n",
    "data['text'] = data['titre']+ \" \" + data['recette']\n",
    "data = data[['text', 'type']]\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "test['text'] = test['titre']+ \" \" + test['recette']\n",
    "test = test[['text', 'type']]\n",
    "test['text'] = test['text'].apply(clean_text)\n",
    "X_testt = test['text']\n",
    "y_testt = test['type']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_testt)\n",
    "\n",
    "svm_classifier = SVC(kernel=\"rbf\",random_state=42)\n",
    "svm_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "y_pred_counts = svm_classifier.predict(X_test_counts)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_testt, y_pred_counts)\n",
    "precision = precision_score(y_testt, y_pred_counts, average='weighted')\n",
    "recall = recall_score(y_testt, y_pred_counts, average='weighted')\n",
    "f1 = f1_score(y_testt, y_pred_counts, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_testt, y_pred_counts)\n",
    "class_report = classification_report(y_testt, y_pred_counts)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8566282420749279\n",
      "Precision: 0.8535724704506641\n",
      "Recall: 0.8566282420749279\n",
      "F1 Score: 0.8539056442284936\n",
      "Confusion Matrix:\n",
      "[[402   3   2]\n",
      " [  4 219 114]\n",
      " [  5  71 568]]\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Dessert       0.98      0.99      0.98       407\n",
      "        Entrée       0.75      0.65      0.70       337\n",
      "Plat principal       0.83      0.88      0.86       644\n",
      "\n",
      "      accuracy                           0.86      1388\n",
      "     macro avg       0.85      0.84      0.84      1388\n",
      "  weighted avg       0.85      0.86      0.85      1388\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #Modèle Word2Vec\n",
    "\n",
    "data = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Prétraitement des données d'entraînement\n",
    "data['text'] = data['titre']+ \" \" + data['recette']\n",
    "data = data[['text', 'type']]\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "test['text'] = test['titre']+ \" \" + test['recette']\n",
    "test = test[['text', 'type']]\n",
    "test['text'] = test['text'].apply(clean_text)\n",
    "X_testt = test['text']\n",
    "y_testt = test['type']\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "corpus = [sentence.split() for sentence in X_train]\n",
    "word2vec_model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Calcul des vecteurs de mots moyens pour chaque document\n",
    "def document_vector(word2vec_model, doc):\n",
    "    \"\"\"Calculer le vecteur moyen pour un document en utilisant les vecteurs de mots Word2Vec\"\"\"\n",
    "    # filtrer les mots absents dans le vocabulaire\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    if len(doc) != 0:\n",
    "        return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "X_train_word2vec = np.array([document_vector(word2vec_model, doc.split()) for doc in X_train])\n",
    "X_test_word2vec = np.array([document_vector(word2vec_model, doc.split()) for doc in X_testt])\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', C=10, random_state=42)\n",
    "svm_classifier.fit(X_train_word2vec, y_train)\n",
    "\n",
    "\n",
    "y_pred_word2vec = svm_classifier.predict(X_test_word2vec)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_testt, y_pred_word2vec)\n",
    "precision = precision_score(y_testt, y_pred_word2vec, average='weighted')\n",
    "recall = recall_score(y_testt, y_pred_word2vec, average='weighted')\n",
    "f1 = f1_score(y_testt, y_pred_word2vec, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_testt, y_pred_word2vec)\n",
    "class_report = classification_report(y_testt, y_pred_word2vec)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### TEST Hyperpramètres #######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
